{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Pre Processing"],"metadata":{"id":"kUJ63BTggNuo"}},{"cell_type":"markdown","source":["Initial Pre-Processing of data"],"metadata":{"id":"ocJrf13ezIPf"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"d8UC1Q-7f2Lh","executionInfo":{"status":"error","timestamp":1761436737593,"user_tz":240,"elapsed":40,"user":{"displayName":"David Berry","userId":"10699397789555731076"}},"colab":{"base_uri":"https://localhost:8080/","height":332},"outputId":"217f70f0-2d28-441a-ec27-1d5de45beecc"},"outputs":[{"output_type":"error","ename":"UnpicklingError","evalue":"pickle data was truncated","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1842294347.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load raw data from pickle into a dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'raw_data.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Remove useless columns: anime_url, image_url, scored_by, favorites, rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    200\u001b[0m                     \u001b[0;31m# We want to silence any warnings about, e.g. moved modules.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;31m# e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnpicklingError\u001b[0m: pickle data was truncated"]}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import pickle\n","\n","# Load raw data from pickle into a dataframe\n","df = pd.read_pickle('raw_data.pkl')\n","\n","# Remove useless columns: anime_url, image_url, scored_by, favorites, rank\n","columns_to_drop = [\n","    'anime_url', 'image_url', 'scored_by', 'favorites', 'rank',\n","    ]\n","df = df.drop(columns=columns_to_drop, axis=1)\n","\n","# Remove Special Characters\n","def clean_text(text):\n","    if pd.isna(text):\n","        return ''\n","    text = text.lower()\n","    text = re.sub(r'[^a-z0-9\\s]', ' ', text)   # remove punctuation and symbols\n","    text = re.sub(r'\\s+', ' ', text).strip()   # normalize whitespace\n","    return text\n","\n","\n","for col in df.select_dtypes(include='object').columns:\n","    if col == 'synopsis':\n","      continue\n","    df[col] = df[col].astype(str).apply(clean_text)\n","\n","\n","# Change premiered to just year\n","def extract_year(premiered):\n","    if pd.isna(premiered):\n","        return np.nan\n","    match = re.search(r'(19|20)\\d{2}', str(premiered))\n","    return int(match.group(0)) if match else np.nan\n","\n","df['premiered'] = df['premiered'].apply(extract_year)\n","\n","# Change duration to an integer of total minutes\n","def parse_duration(duration):\n","    if pd.isna(duration):\n","        return np.nan\n","    duration = duration.lower()\n","\n","    hours = re.search(r'(\\d+)\\s*hr', duration)\n","    mins = re.search(r'(\\d+)\\s*min', duration)\n","\n","    total_minutes = 0\n","    if hours:\n","        total_minutes += int(hours.group(1)) * 60\n","    if mins:\n","        total_minutes += int(mins.group(1))\n","\n","    return total_minutes if total_minutes > 0 else np.nan\n","\n","df['duration'] = df['duration'].apply(parse_duration)\n","\n","# Make numbers integers\n","df['score'] = pd.to_numeric(df['score'], errors='coerce')\n","df['episodes'] = pd.to_numeric(df['episodes'], errors='coerce')\n","\n","# Clipping data at the 99th percentile to stop outliers\n","cols_to_clip = ['members', 'popularity', 'episodes']\n","\n","for col in cols_to_clip:\n","    upper = df[col].quantile(0.99)\n","    lower = df[col].quantile(0.01)\n","    df[col] = df[col].clip(lower=lower, upper=upper)\n","\n","# Fill missing data with blanks\n","df = df.fillna({\n","    'english_name': '',\n","    'japanese_names': '',\n","    'genres': '',\n","    'themes': '',\n","    'demographics': '',\n","    'duration': df['duration'].median(),\n","    'synopsis': '',\n","    'episodes': df['episodes'].median(),\n","    'studios': '',\n","    'source': '',\n","    'producers': '',\n","    'premiered': 0,\n","    'type': '',\n","    'rating': '',\n","})\n","\n","# Drop useless, almost blank animes\n","df = df.dropna(subset=['name', 'score'])\n","df = df[(df['genres'] != '') | (df['synopsis'] != '')]\n","\n","# Drop duplicate anime entries based on anime_id\n","df = df.drop_duplicates(subset='anime_id', keep='first').reset_index(drop=True)\n","\n","# Cleaning up spacing and formatting of some data\n","# Making commas spaced evenly, forcing all to lowercase\n","def clean_list_column(text):\n","    if pd.isna(text): return ''\n","    return ', '.join([x.strip().lower() for x in str(text).split(',')])\n","\n","# Applying formatting rules to necessary areas\n","for col in ['genres', 'themes', 'demographics', 'studios']:\n","    df[col] = df[col].apply(clean_list_column)\n","\n","# Saving Cleaned Data\n","df.to_pickle('cleaned_data.pkl')\n","df.to_parquet(\"cleaned_data.parquet\", engine='pyarrow', compression='snappy')\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5z6OS3vVxWtf","executionInfo":{"status":"ok","timestamp":1761402524446,"user_tz":240,"elapsed":1604,"user":{"displayName":"David Berry","userId":"10699397789555731076"}},"outputId":"49772c01-cde1-4fea-c35f-0a64d7fe680b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from rapidfuzz import fuzz\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.preprocessing import normalize\n","from scipy.sparse import hstack\n","\n","# === 1️⃣ Load cleaned data ===\n","df = pd.read_parquet(\"cleaned_data.parquet\")\n","\n","# (Optional) Ensure text columns are strings\n","for col in ['genres', 'themes', 'demographics', 'synopsis', 'studios', 'type', 'rating']:\n","    df[col] = df[col].astype(str)\n","\n","# === 2️⃣ Build TF-IDF vectorizers for different fields ===\n","vectorizer_genres = TfidfVectorizer(stop_words='english')\n","vectorizer_themes = TfidfVectorizer(stop_words='english')\n","vectorizer_synopsis = TfidfVectorizer(stop_words='english', max_features=10000)\n","vectorizer_studios = TfidfVectorizer(stop_words='english')\n","\n","# Fit & transform each field\n","tfidf_genres = vectorizer_genres.fit_transform(df['genres'])\n","tfidf_themes = vectorizer_themes.fit_transform(df['themes'])\n","tfidf_synopsis = vectorizer_synopsis.fit_transform(df['synopsis'])\n","tfidf_studios = vectorizer_studios.fit_transform(df['studios'])\n","\n","# Normalize to make sure scales are comparable\n","tfidf_genres = normalize(tfidf_genres)\n","tfidf_themes = normalize(tfidf_themes)\n","tfidf_synopsis = normalize(tfidf_synopsis)\n","tfidf_studios = normalize(tfidf_studios)\n","\n","# === 3️⃣ Set adjustable weights for each feature group ===\n","W_GENRES  = 2.0    # importance of genre similarity\n","W_THEMES  = 1.5    # importance of themes\n","W_SYNOPSIS = 1.0   # importance of synopsis\n","W_STUDIOS = 0.5    # importance of studio similarity\n","\n","# Weighted combination of TF-IDF matrices\n","combined_matrix = hstack([\n","    W_GENRES * tfidf_genres,\n","    W_THEMES * tfidf_themes,\n","    W_SYNOPSIS * tfidf_synopsis,\n","    W_STUDIOS * tfidf_studios\n","])\n","\n","def extract_core_name(title: str) -> str:\n","    \"\"\"\n","    Extracts a normalized franchise/base title.\n","    This removes sequel indicators, subtitles, punctuation, and numbers.\n","    \"\"\"\n","    if pd.isna(title):\n","        return ''\n","    title = title.lower()\n","\n","    # Cut at common subtitle separators\n","    title = re.split(r'[:\\-–—\\(\\)\\[\\]]', title)[0]\n","\n","    # Remove sequel indicators\n","    title = re.sub(r'\\b(season|part|special|movie|ova|final|chapter|edition|arc|remake|rebuild|stage|act|sequel|prequel|series)\\b', '', title)\n","\n","    # Remove ordinal indicators like 1st, 2nd, 3rd, etc.\n","    title = re.sub(r'\\b(\\d+(st|nd|rd|th))\\b', '', title)\n","\n","    # Remove standalone digits and roman numerals (I, II, III, IV, etc.)\n","    title = re.sub(r'\\b(\\d+|i{1,3}|iv|v|vi{0,3}|vii{0,3}|ix|x)\\b', '', title)\n","\n","    # Remove extra words like \"hyouketsu no kizuna\", \"divide by zero\"\n","    # (They are post-subtitle phrases in many MAL entries)\n","    title = re.sub(r'(no|by|kizuna|link|divide|zero|hyouketsu)', '', title)\n","\n","    # Remove punctuation, normalize whitespace\n","    title = re.sub(r'[^a-z0-9\\s]', ' ', title)\n","    title = re.sub(r'\\s+', ' ', title).strip()\n","\n","    return title\n","\n","df[\"core_name\"] = df[\"name\"].apply(extract_core_name)\n","\n","# === 4️⃣ Compute cosine similarity ===\n","cosine_sim = cosine_similarity(combined_matrix, combined_matrix)\n","\n","print(\"✅ Combined TF-IDF matrix and cosine similarity computed.\")\n","\n","\n","# === 5️⃣ Recommendation function ==\n","def recommend_anime_by_id(anime_id, top_n=5):\n","    \"\"\"Recommend similar anime given its unique anime_id, excluding sequels/franchise duplicates.\"\"\"\n","    if anime_id not in df['anime_id'].values:\n","        return f\"No anime found with id {anime_id}.\"\n","\n","    idx = df.index[df['anime_id'] == anime_id][0]\n","    base_name = df.loc[idx, 'name']\n","    base_core = df.loc[idx, 'core_name']\n","\n","    sim_scores = list(enumerate(cosine_sim[idx]))\n","    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n","\n","    recs = []\n","    for i, _ in sim_scores:\n","        if i == idx:\n","            continue\n","\n","        cand_core = df.loc[i, 'core_name']\n","        cand_name = df.loc[i, 'name']\n","\n","        # Skip same or very similar franchises\n","        if cand_core == base_core:\n","            continue\n","        if base_core in cand_core or cand_core in base_core:\n","            continue\n","        if fuzz.partial_ratio(base_core, cand_core) > 75:\n","            continue\n","        if re.search(r'(season|part|ova|special|movie|final|rebuild)', cand_name, re.I):\n","            if base_core.split()[0] in cand_core:\n","                continue\n","\n","        recs.append(i)\n","        if len(recs) >= top_n * 3:  # collect a few extra to filter later\n","            break\n","\n","    results = df.iloc[recs][['anime_id', 'name', 'core_name', 'score', 'genres', 'rating']]\n","    results = results.drop_duplicates(subset='core_name', keep='first').head(top_n)\n","\n","    # ✅ Return only the clean version too\n","    return results[['anime_id', 'name', 'score']].reset_index(drop=True)\n","\n","print(recommend_anime_by_id(9253, top_n=5))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o_EbHMNeDN1g","executionInfo":{"status":"ok","timestamp":1761414704744,"user_tz":240,"elapsed":8161,"user":{"displayName":"David Berry","userId":"10699397789555731076"}},"outputId":"b6bc67be-500f-4116-ee8f-76669fc2f405"},"execution_count":104,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Combined TF-IDF matrix and cosine similarity computed.\n","   anime_id                                               name  score\n","0     42203  re zero kara hajimeru isekai seikatsu 2nd seas...   8.42\n","1     49241  vivy fluorite eye s song to make everyone happ...   7.31\n","2     46095                           vivy fluorite eye s song   8.38\n","3       385                                          gilgamesh   6.59\n","4     39584                         human lost ningen shikkaku   5.78\n"]}]},{"cell_type":"markdown","source":["Preprocessing w/ out name bias"],"metadata":{"id":"7X3h6bjnSmXc"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import pickle\n","\n","# Load raw data from pickle into a dataframe\n","df = pd.read_pickle('raw_data.pkl')\n","\n","# Remove useless columns: anime_url, image_url, scored_by, favorites, rank, and both names\n","columns_to_drop = [\n","    'anime_url', 'image_url', 'scored_by', 'favorites', 'rank', 'english_name', 'japanese_names'\n","    ]\n","df = df.drop(columns=columns_to_drop, axis=1)\n","\n","print(df.shape)\n","\n","# Remove Special Characters\n","def clean_text(text):\n","    if pd.isna(text):\n","        return ''\n","    text = text.lower()\n","    text = re.sub(r'[^a-z0-9\\s]', ' ', text)   # remove punctuation and symbols\n","    text = re.sub(r'\\s+', ' ', text).strip()   # normalize whitespace\n","    return text\n","\n","\n","for col in df.select_dtypes(include='object').columns:\n","    if col == 'synopsis':\n","      continue\n","    df[col] = df[col].astype(str).apply(clean_text)\n","\n","\n","# Change premiered to just year\n","def extract_year(premiered):\n","    if pd.isna(premiered):\n","        return np.nan\n","    match = re.search(r'(19|20)\\d{2}', str(premiered))\n","    return int(match.group(0)) if match else np.nan\n","\n","df['premiered'] = df['premiered'].apply(extract_year)\n","\n","# Change duration to an integer of total minutes\n","def parse_duration(duration):\n","    if pd.isna(duration):\n","        return np.nan\n","    duration = duration.lower()\n","\n","    hours = re.search(r'(\\d+)\\s*hr', duration)\n","    mins = re.search(r'(\\d+)\\s*min', duration)\n","\n","    total_minutes = 0\n","    if hours:\n","        total_minutes += int(hours.group(1)) * 60\n","    if mins:\n","        total_minutes += int(mins.group(1))\n","\n","    return total_minutes if total_minutes > 0 else np.nan\n","\n","df['duration'] = df['duration'].apply(parse_duration)\n","\n","# Make numbers integers\n","df['score'] = pd.to_numeric(df['score'], errors='coerce')\n","df['episodes'] = pd.to_numeric(df['episodes'], errors='coerce')\n","\n","# Clipping data at the 99th percentile to stop outliers\n","cols_to_clip = ['members', 'popularity', 'episodes']\n","\n","for col in cols_to_clip:\n","    upper = df[col].quantile(0.99)\n","    lower = df[col].quantile(0.01)\n","    df[col] = df[col].clip(lower=lower, upper=upper)\n","\n","# Fill missing data with blanks\n","df = df.fillna({\n","    'genres': '',\n","    'themes': '',\n","    'demographics': '',\n","    'duration': df['duration'].median(),\n","    'synopsis': '',\n","    'episodes': df['episodes'].median(),\n","    'studios': '',\n","    'source': '',\n","    'producers': '',\n","    'premiered': 0,\n","    'type': '',\n","    'rating': '',\n","})\n","\n","# Drop useless, almost blank animes\n","df = df.dropna(subset=['score'])\n","df = df[(df['genres'] != '') | (df['synopsis'] != '')]\n","df = df[(df['type'] == 'tv') | (df['type'] == 'movie') | (df['type'] == 'ona') | (df['type'] == 'ova')]\n","df = df[~((df[\"type\"] != \"movie\") & (df[\"episodes\"] < df['episodes'].median()))]\n","df = df[(df['score'] > 5)]\n","df = df[~df[\"genres\"].str.contains(\"hentai\", case=False, na=False)]\n","df = df[~df[\"genres\"].str.contains(\"erotica\", case=False, na=False)]\n","\n","# Drop duplicate anime entries based on anime_id\n","df = df.drop_duplicates(subset='anime_id', keep='first').reset_index(drop=True)\n","\n","# Cleaning up spacing and formatting of some data\n","# Making commas spaced evenly, forcing all to lowercase\n","def clean_list_column(text):\n","    if pd.isna(text): return ''\n","    return ', '.join([x.strip().lower() for x in str(text).split(',')])\n","\n","# Applying formatting rules to necessary areas\n","for col in ['genres', 'themes', 'demographics', 'studios']:\n","    df[col] = df[col].apply(clean_list_column)\n","\n","# Saving Cleaned Data\n","df.to_pickle('cleaned_data.pkl')\n","df.to_parquet(\"cleaned_data.parquet\", engine='pyarrow', compression='snappy')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KsJ8N0i0Xy01","executionInfo":{"status":"ok","timestamp":1761462253283,"user_tz":240,"elapsed":937,"user":{"displayName":"David Berry","userId":"10699397789555731076"}},"outputId":"7ed60048-2e6b-4199-d80b-6488a013d362"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["(15000, 17)\n"]}]},{"cell_type":"code","source":["df = pd.read_csv('raw_data.csv')\n","df.to_pickle('raw_data.pkl')"],"metadata":{"id":"KPhuP1bq0IoR","executionInfo":{"status":"ok","timestamp":1761455599768,"user_tz":240,"elapsed":355,"user":{"displayName":"David Berry","userId":"10699397789555731076"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["filtered_df = df[df['anime_id'] == 21]\n","print(filtered_df.head)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IDW6HuKQJBNt","executionInfo":{"status":"ok","timestamp":1761459073185,"user_tz":240,"elapsed":25,"user":{"displayName":"David Berry","userId":"10699397789555731076"}},"outputId":"ec230198-2304-4451-87d8-f9170d412dd2"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["<bound method NDFrame.head of Empty DataFrame\n","Columns: [anime_id, score, genres, themes, demographics, synopsis, type, episodes, premiered, producers, studios, source, duration, rating, popularity, members]\n","Index: []>\n"]}]}]}